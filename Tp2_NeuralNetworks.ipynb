{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Multi-Layer Perceptron and Convolutional Neural Networks\n",
    "## Data-Analysis: Lab 2\n",
    "\n",
    "Simon ROBER, simon.rober@grenoble-inp.org, March 28 2020, time spend on TP: 9 hours\n",
    "## Part I: Multi-Layer Perceptron with sklearn\n",
    "\n",
    "### 1 Learning Boolean Operators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: AND\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Inputs\n",
    "y = [0, 0, 0, 1] # Outputs\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='identity', solver='lbfgs')\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) #expect a perfect prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Inputs\n",
    "y = [0, 1, 1, 1] # Outputs\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='identity', solver='lbfgs')\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) #expect a perfect prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: XOR\n",
    "###### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0]\n",
      "[1 1 0 1]\n",
      "[1 0 1 0]\n",
      "[1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Inputs\n",
    "y = [0, 1, 1, 0] # Outputs\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='identity', solver='lbfgs')\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X))\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) \n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) \n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the classifier always gives different results every time we train with the same training set, hence it classifies incorrect. It is because we use a linear activation function, so it cannot seperate the data of an XOR operation.\n",
    "\n",
    "###### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n",
      "[0 0 0 0]\n",
      "[0 1 0 1]\n",
      "[1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Inputs\n",
    "y = [0, 1, 1, 0] # Outputs\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(4,2,), activation='identity', solver='lbfgs')\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X))\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) \n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) \n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the classifier always gives different results every time we train with the same training set, hence it still classifies incorrect. Now by adding in hidden nodes we do not alter the ability for the classifier to seperate non-linear problem like to XOR. This is the same as when we did not add any hidden layers.\n",
    "\n",
    "###### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n",
      "[0 1 1 0]\n",
      "[0 1 1 0]\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ] # Inputs\n",
    "y = [0, 1, 1, 0] # Outputs\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(4,2,), activation='tanh', solver='lbfgs')\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X))\n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) \n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) \n",
    "classifier.fit(X, y)\n",
    "print(classifier.predict(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results are correct! This occurs because the activation function is non linear. Hence by adding a non-linear element to the classifier in the activation function. The classifier can now classify non-linear problems.\n",
    "\n",
    "### 2 Image Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = load_digits()\n",
    "X = dataset.data # Inputs\n",
    "y = dataset.target # Associated outputs\n",
    "train_X , test_X , train_y , test_y = train_test_split(X, y , test_size =0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section of code you will find the same instruction with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='identity', solver='lbfgs')\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(50,25,), activation='identity', solver='lbfgs', max_iter=1000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(100,50,), activation='identity', solver='lbfgs', max_iter=2000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.95\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='identity', solver='lbfgs', max_iter=1000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='tanh', solver='lbfgs', max_iter=500)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(50,25,), activation='tanh', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,), activation='tanh', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,50,), activation='tanh', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.95\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='relu', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(100,50,), activation='relu', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,), activation='relu', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,50), activation='relu', solver='lbfgs', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.95\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='relu', solver='sgd', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,), activation='relu', solver='sgd', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(), activation='relu', solver='adam', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,), activation='relu', solver='adam', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy :  0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(500,100,50), activation='relu', solver='adam', max_iter=20000)\n",
    "classifier.fit(train_X , train_y)\n",
    "test_y_pred = classifier.predict(test_X) # Predicted results\n",
    "print ( \" Accuracy : \" , accuracy_score ( test_y , test_y_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing around for a while, I tried to make some conclusions:\n",
    "* Accuracies don't differ to much between the different networks. (0.022 at most)\n",
    "* The more neurons per layer the better the accuracy.\n",
    "* There are a lot of combinations of parameters to play with ( this list was not exhaustive).\n",
    "* Adding an extra layer doesn't always increase the accuracy.\n",
    "* The best combination from the list are: (500, 100) tanh lbfsg and (500,100,50) relu adam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: PyTorch and convolutional neural nets\n",
    "\n",
    "### LeNet5 network for the MNIST Database using torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading and normalizing MNIST\n",
    "\n",
    "Import the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three',\n",
    "           'four', 'five', 'six', 'seven', 'eight', 'nine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few images with there corresponding labes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd0a287a58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight  zero   one seven\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3,stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3,stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)       # Dense / Fully connnected layer.\n",
    "        self.fc2 = nn.Linear(120, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))                           # Flatten, should be 400.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net1 = Net1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the network\n",
    "\n",
    "Note here that there is written 4 epochs. This takes a very long time ... . Feel free to adjust it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, criterion, optimizer):\n",
    "    for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.143\n",
      "[1,  4000] loss: 0.348\n",
      "[1,  6000] loss: 0.256\n",
      "[1,  8000] loss: 0.191\n",
      "[1, 10000] loss: 0.160\n",
      "[1, 12000] loss: 0.149\n",
      "[1, 14000] loss: 0.129\n"
     ]
    }
   ],
   "source": [
    "train(trainloader, net1, criterion1, optimizer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prAccuracyGeneral(testloader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prAccuracyEachClass(testloader, net):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96 %\n",
      "Accuracy of  zero : 98 %\n",
      "Accuracy of   one : 97 %\n",
      "Accuracy of   two : 97 %\n",
      "Accuracy of three : 97 %\n",
      "Accuracy of  four : 95 %\n",
      "Accuracy of  five : 93 %\n",
      "Accuracy of   six : 96 %\n",
      "Accuracy of seven : 95 %\n",
      "Accuracy of eight : 98 %\n",
      "Accuracy of  nine : 94 %\n"
     ]
    }
   ],
   "source": [
    "prAccuracyGeneral(testloader, net1)\n",
    "prAccuracyEachClass(testloader, net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the accuracy and compare it to the MLP accuracy; comment:\n",
    "\n",
    "(Epochs = 4) Given enough epochs, the overal accuracy of the CNN (98%) is as good and the best MLP (98%). When we look at the accuracy of the individual digits, it sometimes even goes up to 99%.\n",
    "This makes CNN's superior in terms of accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the network architecture (size of feature maps, size of kernel); comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.270\n",
      "[1,  4000] loss: 0.304\n",
      "[1,  6000] loss: 0.189\n",
      "[1,  8000] loss: 0.149\n",
      "[1, 10000] loss: 0.132\n",
      "[1, 12000] loss: 0.114\n",
      "[1, 14000] loss: 0.095\n",
      "Accuracy of the network on the 10000 test images: 97 %\n",
      "Accuracy of  zero : 98 %\n",
      "Accuracy of   one : 99 %\n",
      "Accuracy of   two : 96 %\n",
      "Accuracy of three : 98 %\n",
      "Accuracy of  four : 95 %\n",
      "Accuracy of  five : 96 %\n",
      "Accuracy of   six : 97 %\n",
      "Accuracy of seven : 97 %\n",
      "Accuracy of eight : 95 %\n",
      "Accuracy of  nine : 98 %\n"
     ]
    }
   ],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5,stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5,stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(4*4*16, 120)       # Dense / Fully connnected layer.\n",
    "        self.fc2 = nn.Linear(120, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))                           # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net2 = Net2()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)\n",
    "train(trainloader, net2, criterion2, optimizer2)\n",
    "prAccuracyGeneral(testloader, net2)\n",
    "prAccuracyEachClass(testloader, net2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Epochs = 1, if epochs would be 4, you don't see the difference in accuracy) By increasing the kernel size from 3 to 5, we decrease the size of the feature map. This result in a decrease of accuracy from 97% to 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the learning rate:\n",
    "\n",
    "First we devide the learning rate by ten, then we multiply it by 10 in order to see how the accuracy is effected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.301\n",
      "[1,  4000] loss: 2.292\n",
      "[1,  6000] loss: 2.272\n",
      "[1,  8000] loss: 2.183\n",
      "[1, 10000] loss: 1.232\n",
      "[1, 12000] loss: 0.527\n",
      "[1, 14000] loss: 0.463\n",
      "[2,  2000] loss: 0.367\n",
      "[2,  4000] loss: 0.333\n",
      "[2,  6000] loss: 0.325\n",
      "[2,  8000] loss: 0.297\n",
      "[2, 10000] loss: 0.284\n",
      "[2, 12000] loss: 0.259\n",
      "[2, 14000] loss: 0.263\n",
      "[3,  2000] loss: 0.237\n",
      "[3,  4000] loss: 0.200\n",
      "[3,  6000] loss: 0.225\n",
      "[3,  8000] loss: 0.204\n",
      "[3, 10000] loss: 0.181\n",
      "[3, 12000] loss: 0.183\n",
      "[3, 14000] loss: 0.169\n",
      "[4,  2000] loss: 0.164\n",
      "[4,  4000] loss: 0.168\n",
      "[4,  6000] loss: 0.165\n",
      "[4,  8000] loss: 0.157\n",
      "[4, 10000] loss: 0.151\n",
      "[4, 12000] loss: 0.138\n",
      "[4, 14000] loss: 0.135\n",
      "Accuracy of the network on the 10000 test images: 96 %\n",
      "Accuracy of  zero : 99 %\n",
      "Accuracy of   one : 99 %\n",
      "Accuracy of   two : 96 %\n",
      "Accuracy of three : 96 %\n",
      "Accuracy of  four : 97 %\n",
      "Accuracy of  five : 92 %\n",
      "Accuracy of   six : 97 %\n",
      "Accuracy of seven : 96 %\n",
      "Accuracy of eight : 92 %\n",
      "Accuracy of  nine : 92 %\n"
     ]
    }
   ],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3,stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3,stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)       # Dense / Fully connnected layer.\n",
    "        self.fc2 = nn.Linear(120, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))                           # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net3 = Net3()\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "optimizer3 = optim.SGD(net3.parameters(), lr=0.0001, momentum=0.9)\n",
    "train(trainloader, net3, criterion3, optimizer3)\n",
    "prAccuracyGeneral(testloader, net3)\n",
    "prAccuracyEachClass(testloader, net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.599\n",
      "[1,  4000] loss: 0.248\n",
      "[1,  6000] loss: 0.166\n",
      "[1,  8000] loss: 0.141\n",
      "[1, 10000] loss: 0.140\n",
      "[1, 12000] loss: 0.131\n",
      "[1, 14000] loss: 0.133\n",
      "[2,  2000] loss: 0.102\n",
      "[2,  4000] loss: 0.095\n",
      "[2,  6000] loss: 0.087\n",
      "[2,  8000] loss: 0.100\n",
      "[2, 10000] loss: 0.090\n",
      "[2, 12000] loss: 0.094\n",
      "[2, 14000] loss: 0.088\n",
      "[3,  2000] loss: 0.070\n",
      "[3,  4000] loss: 0.077\n",
      "[3,  6000] loss: 0.091\n",
      "[3,  8000] loss: 0.072\n",
      "[3, 10000] loss: 0.078\n",
      "[3, 12000] loss: 0.071\n",
      "[3, 14000] loss: 0.076\n",
      "[4,  2000] loss: 0.067\n",
      "[4,  4000] loss: 0.063\n",
      "[4,  6000] loss: 0.074\n",
      "[4,  8000] loss: 0.067\n",
      "[4, 10000] loss: 0.068\n",
      "[4, 12000] loss: 0.078\n",
      "[4, 14000] loss: 0.071\n",
      "Accuracy of the network on the 10000 test images: 97 %\n",
      "Accuracy of  zero : 99 %\n",
      "Accuracy of   one : 99 %\n",
      "Accuracy of   two : 97 %\n",
      "Accuracy of three : 96 %\n",
      "Accuracy of  four : 98 %\n",
      "Accuracy of  five : 96 %\n",
      "Accuracy of   six : 95 %\n",
      "Accuracy of seven : 98 %\n",
      "Accuracy of eight : 98 %\n",
      "Accuracy of  nine : 98 %\n"
     ]
    }
   ],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3,stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3,stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)       # Dense / Fully connnected layer.\n",
    "        self.fc2 = nn.Linear(120, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))                           # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net4 = Net4()\n",
    "criterion4 = nn.CrossEntropyLoss()\n",
    "optimizer4 = optim.SGD(net4.parameters(), lr=0.01, momentum=0.9)\n",
    "train(trainloader, net4, criterion4, optimizer4)\n",
    "prAccuracyGeneral(testloader, net4)\n",
    "prAccuracyEachClass(testloader, net4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Epochs = 4) We see that the accuarcy drop 98% to 96% when decreasing the learning rate and from 98% to 97% when increasing the learning rate. This is sort of the be expected considering that the learning rate may not be to big, nor to small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
